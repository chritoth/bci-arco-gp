{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and store benchmark environments\n",
    "\n",
    "In this notebook can be used to generate benchmark environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from src.environments.generic_environments import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% imports\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate the environments."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# setup\n",
    "env_class = BarabasiAlbert\n",
    "num_envs = 20\n",
    "num_nodes = 50\n",
    "delete_existing = False  # delete existing benchmarks\n",
    "output_dir = f'../data/{env_class.__name__}/{num_nodes}_nodes_100_train_linear/'  # dir where to store the generated envs\n",
    "\n",
    "# generate/empty folder for envs of same type\n",
    "if os.path.isdir(output_dir) and not delete_existing:\n",
    "    print('\\nDirectory \\'' + output_dir + '\\' already exists, not generating benchmarks...')\n",
    "else:\n",
    "    if os.path.isdir(output_dir):\n",
    "        print('\\nDirectory \\'' + output_dir + '\\' already exists, delete existing benchmarks...')\n",
    "        for root, dirs, files in os.walk(output_dir):\n",
    "            for file in files:\n",
    "                os.remove(os.path.join(root, file))\n",
    "            for folder in dirs:\n",
    "                shutil.rmtree(os.path.join(root, folder))\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # generate benchmark envs\n",
    "    for i in range(num_envs):\n",
    "        os.system(f'cp ../src/config.py {output_dir}config.py')\n",
    "        env = env_class(num_nodes=num_nodes)\n",
    "        env_path = output_dir + env.name + '.pth'\n",
    "        env.save(env_path)\n",
    "        print(f'\\rGenerated {i + 1}/{num_envs} environments.', end='')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Take existing environments, modify them and store them seperately."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# setup\n",
    "env_class = ErdosRenyi\n",
    "delete_existing = False  # delete existing benchmarks\n",
    "source_dir = f'../data/{env_class.__name__}/20_nodes_100_train'  # dir where origianl envs are stored\n",
    "target_dir = f'../data/{env_class.__name__}/20_nodes_100_train_normalised/'  # dir where to store the generated envs\n",
    "\n",
    "\n",
    "# you can implement modifications here\n",
    "def modify_env(env: Environment):\n",
    "    # normalise train and test data\n",
    "    env.cfg.normalise_data = True\n",
    "\n",
    "    env.normalisation_means = {}\n",
    "    env.normalisation_stds = {}\n",
    "    for node, values in env.observational_train_data[0].data.items():\n",
    "        env.normalisation_means[node] = values.mean()\n",
    "        env.normalisation_stds[node] = values.std()\n",
    "\n",
    "    env.observational_train_data[0].normalise(env.normalisation_means, env.normalisation_stds)\n",
    "\n",
    "    for eidx, exp in enumerate(env.interventional_train_data):\n",
    "        exp.normalise(env.normalisation_means, env.normalisation_stds)\n",
    "\n",
    "    for eidx, exp in enumerate(env.observational_test_data):\n",
    "        exp.normalise(env.normalisation_means, env.normalisation_stds)\n",
    "\n",
    "    for experiments in env.interventional_test_data.values():\n",
    "        for exp in experiments:\n",
    "            exp.normalise(env.normalisation_means, env.normalisation_stds)\n",
    "\n",
    "    return env\n",
    "\n",
    "\n",
    "# check if source envs available\n",
    "if not os.path.isdir(source_dir):\n",
    "    print(f'Source directory {source_dir} does not exist!')\n",
    "elif os.path.isdir(target_dir) and not delete_existing:\n",
    "    print('\\nTarget directory \\'' + target_dir + '\\' already exists, not generating benchmarks...')\n",
    "else:\n",
    "    # generate/empty folder for target envs\n",
    "    if os.path.isdir(target_dir):\n",
    "        print('\\nTarget directory \\'' + target_dir + '\\' already exists, delete existing benchmarks...')\n",
    "        for root, dirs, files in os.walk(target_dir):\n",
    "            for file in files:\n",
    "                os.remove(os.path.join(root, file))\n",
    "            for folder in dirs:\n",
    "                shutil.rmtree(os.path.join(root, folder))\n",
    "\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    # load source envs\n",
    "    env_files = [entry for entry in os.scandir(source_dir) if\n",
    "                 entry.is_file() and os.path.basename(entry)[-4:] == '.pth']\n",
    "    for i, f in enumerate(env_files):\n",
    "        env = env_class.load(os.path.abspath(f))\n",
    "        modified_env = modify_env(env)\n",
    "        modified_env.save(target_dir + os.path.basename(f))\n",
    "\n",
    "        print(f'\\rProcessed {i + 1}/{len(env_files)} environments in {source_dir}.', end='')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Export .csv dataset from existing environments."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "env_class = BarabasiAlbert\n",
    "data_dir = f'../data/'\n",
    "out_dir = f'../csv_data/'\n",
    "sub_dirs = [f'{env_class.__name__}/20_nodes_100_train/',\n",
    "            f'{env_class.__name__}/20_nodes_200_train/',\n",
    "            ]\n",
    "\n",
    "for sub_dir in sub_dirs:\n",
    "    env_files = [os.path.abspath(entry) for entry in os.scandir(os.path.join(data_dir, sub_dir)) if\n",
    "                 entry.is_file() and os.path.basename(entry)[-4:] == '.pth']\n",
    "    num_environments = len(env_files)\n",
    "\n",
    "    tmp_dir = os.path.join(out_dir, sub_dir)\n",
    "    os.makedirs(tmp_dir, exist_ok=True)\n",
    "    for fidx, env_file in enumerate(env_files):\n",
    "        print(f'\\rExporting {fidx + 1}/{len(env_files)} datasets for environment {os.path.basename(env_file)}.', end='')\n",
    "\n",
    "        Environment.export_csv_dataset(env_file, tmp_dir)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Export .csv dataset for BayesDAG from existing .csv datasets.",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "env_class = ErdosRenyi\n",
    "data_dir = f'../csv_data/'\n",
    "out_dir = f'../bayesdag_data/'\n",
    "sub_dirs = [f'{env_class.__name__}/20_nodes_100_train/',\n",
    "            f'{env_class.__name__}/20_nodes_200_train/',\n",
    "            ]\n",
    "\n",
    "for sub_dir in sub_dirs:\n",
    "    files = [os.path.abspath(entry) for entry in os.scandir(os.path.join(data_dir, sub_dir)) if\n",
    "             entry.is_file() and os.path.basename(entry)[-4:] == '.csv']\n",
    "    num_environments = len(files)\n",
    "\n",
    "    tmp_dir = os.path.join(out_dir, sub_dir)\n",
    "    os.makedirs(tmp_dir, exist_ok=True)\n",
    "    for fidx, file in enumerate(files):\n",
    "        print(f'\\rExporting {fidx + 1}/{len(files)} datasets for environment {os.path.basename(file)}.', end='')\n",
    "        df = pd.read_csv(file)\n",
    "        outpath = os.path.join(tmp_dir, os.path.basename(file))\n",
    "        df.to_csv(outpath, index=False, header=False)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
