{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Estimating average causal effects\n",
    "\n",
    "This notebook illustrates how to estimate average causal effects with ArCO-GP."
   ],
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# imports\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "from src.abci_arco_gp import ABCIArCOGP as ABCI\n",
    "from src.config import ABCIArCOGPConfig\n",
    "from src.environments.generic_environments import *\n",
    "from src.utils.causal_orders import *\n",
    "from src.utils.plotting import init_plot_style\n",
    "\n",
    "init_plot_style()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we generate a ground truth environment/SCM.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# init environment\n",
    "num_nodes = 5\n",
    "env_cfg = EnvironmentConfig()\n",
    "env_cfg.num_observational_train_samples = 200\n",
    "env_cfg.normalise_data = True\n",
    "env_cfg.generate_static_intr_dataset = True\n",
    "env_cfg.num_test_interventions = 1\n",
    "env_cfg.num_interventional_test_samples = 100\n",
    "\n",
    "env = BarabasiAlbert(num_nodes, env_cfg)\n",
    "# env = SachsGraphGeneric(env_cfg)\n",
    "\n",
    "# plot true graph\n",
    "nx.draw(env.graph, nx.circular_layout(env.graph), labels=dict(zip(env.graph.nodes, env.graph.nodes)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, we create an ABCI instance with the desired experimental design policy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "cfg = ABCIArCOGPConfig()\n",
    "cfg.policy = 'static-obs-dataset'\n",
    "cfg.max_ps_size = 2\n",
    "cfg.num_arco_steps = 100\n",
    "cfg.num_mc_cos = 50\n",
    "cfg.num_mc_graphs = 5\n",
    "cfg.num_samples_per_graph = 5\n",
    "abci = ABCI(env, cfg)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "We can now run a number of ABCI loops."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "abci.run()\n",
    "\n",
    "# plot loss over experiments\n",
    "ax = plt.figure().gca()\n",
    "plt.plot(abci.stats['arco_loss'], label='arco_loss')\n",
    "plt.xlabel('Number of Steps')\n",
    "plt.ylabel('Loss')\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Print structure learning stats.",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print()\n",
    "print(f\"ESHD {abci.stats['eshd']} vs. ESHD CPDAG {abci.stats['eshd_cpdag']}\")\n",
    "print(f\"True Num E {env.graph.number_of_edges()} vs. E-NUM Edges{abci.stats['enum_edges']}\")\n",
    "print(f\"A-AID {abci.stats['aaid']}   vs. A-AID cpdag {abci.stats['aaid_cpdag']}\")\n",
    "print(f\"P-AID {abci.stats['paid']}   vs. P-AID cpdag {abci.stats['paid_cpdag']}\")\n",
    "print(f\"OSET-AID {abci.stats['oset_aid']} vs. OSET-AID cpdag {abci.stats['oset_aid_cpdag']} \")\n",
    "print(f\"ORDER-AID {abci.stats['order_aid']} \")\n",
    "print()\n",
    "print(f\"F1 {abci.stats['ef1']}     vs. F1 cpdag {abci.stats['ef1_cpdag']}\")\n",
    "print(f\"TPR {abci.stats['etpr']} vs. TPR cpdag {abci.stats['etpr_cpdag']}\")\n",
    "print(f\"TNR {abci.stats['etnr']} vs. TNR cpdag {abci.stats['etnr_cpdag']}\")\n",
    "print(f\"FNR {abci.stats['efnr']} vs. FNR cpdag {abci.stats['efnr_cpdag']}\")\n",
    "print(f\"FPR {abci.stats['efpr']} vs. FPR cpdag {abci.stats['efpr_cpdag']}\")\n",
    "print(f\"AUROC {abci.stats['auroc']}  vs. AUROC CPDAG {abci.stats['auroc_cpdag']}\")\n",
    "print(f\"AUPRC {abci.stats['auprc']}  vs. AUPRC CPDAG {abci.stats['auprc_cpdag']}\")\n",
    "print()\n",
    "print(f\"MMD {abci.stats['mmd']}, L1 {abci.stats['dmae']}, L2 {abci.stats['dmse']}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sample from posterior interventional distribution."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# sampling from the true and predicted interventional distributions\n",
    "target = 'X4'\n",
    "interventions = {'X1': torch.tensor(1.)}\n",
    "num_env_samples = 10000\n",
    "num_samples_per_graph = 10\n",
    "with torch.no_grad():\n",
    "    # sample from true interventional distribution\n",
    "    env_samples = env.sample(interventions, num_env_samples, 1).data[target].squeeze()\n",
    "    env_weights = torch.ones_like(env_samples) / env_samples.numel()\n",
    "    env_mean = env_samples.mean()\n",
    "\n",
    "    # sample from learned posterior distribution\n",
    "    samples, weights = abci.sample(interventions, num_samples_per_graph)\n",
    "    samples = samples[target]\n",
    "    posterior_mean = weights @ samples\n",
    "\n",
    "print(f'E[{target} | do({interventions})] = {env_mean} (TRUE) vs. {posterior_mean} (PREDICTION)')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot the empirical posterior interventional distribution."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# plotting params\n",
    "xrange = (-3., 3.)\n",
    "num_bins = 200\n",
    "bin_width = (xrange[1] - xrange[0]) / num_bins\n",
    "true_color = '#5DADE2'\n",
    "pred_color = '#911225'\n",
    "alpha = 0.3\n",
    "kernel_bw = 0.2\n",
    "lw = 7\n",
    "\n",
    "min_x = min(env_samples.min(), samples.min()).item()\n",
    "max_x = max(env_samples.max(), samples.max()).item()\n",
    "support = torch.linspace(min_x, max_x, steps=500).unsqueeze(-1)\n",
    "\n",
    "xrange = (min_x, max_x)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# true distribution\n",
    "hist, bin_edges = torch.histogram(env_samples, bins=num_bins, range=xrange, weight=env_weights, density=True)\n",
    "plt.bar(bin_edges[:-1], hist, align='edge', width=bin_width, color=true_color, alpha=alpha)\n",
    "\n",
    "kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_bw)\n",
    "kde.fit(env_samples.view(-1, 1), sample_weight=env_weights)\n",
    "log_dens = kde.score_samples(support)\n",
    "plt.plot(support, np.exp(log_dens), '--', color=true_color, lw=lw)\n",
    "\n",
    "weights = weights + 1e-10\n",
    "# predicted distribution\n",
    "hist, bin_edges = torch.histogram(samples, bins=num_bins, range=xrange, weight=weights, density=True)\n",
    "plt.bar(bin_edges[:-1], hist, align='edge', width=bin_width, color=pred_color, alpha=alpha)\n",
    "\n",
    "kde = KernelDensity(kernel=\"gaussian\", bandwidth=kernel_bw)\n",
    "kde.fit(samples.view(-1, 1), sample_weight=weights)\n",
    "log_dens = kde.score_samples(support)\n",
    "plt.plot(support, np.exp(log_dens), color=pred_color, lw=lw)\n",
    "\n",
    "# means\n",
    "eps = 1e-2\n",
    "plt.plot([env_mean - eps, env_mean - eps], [0., max(hist)], '--', color='black', lw=lw)\n",
    "plt.plot([env_mean + eps, env_mean + eps], [0., max(hist)], '--', color='black', lw=lw)\n",
    "plt.plot([env_mean, env_mean], [0., max(hist)], '--', color=true_color, label='True ACE', lw=lw)\n",
    "\n",
    "plt.plot([posterior_mean - eps, posterior_mean - eps], [0., max(hist)], color='black', lw=lw)\n",
    "plt.plot([posterior_mean + eps, posterior_mean + eps], [0., max(hist)], color='black', lw=lw)\n",
    "plt.plot([posterior_mean, posterior_mean], [0., max(hist)], color=pred_color, label='Predicted ACE', lw=lw)\n",
    "\n",
    "# label\n",
    "intr_str = ', '.join([f'do({k}={v.int()})' for k, v in interventions.items()])\n",
    "plt.xlabel(f'{target} | {intr_str}')\n",
    "# plt.xlim([-2.5, 2.5])\n",
    "plt.tight_layout()\n",
    "\n",
    "save_plots = False\n",
    "dpi = 400\n",
    "fig_format = 'pdf'\n",
    "fig_dir = '../figures/'\n",
    "figdate = '20241011'\n",
    "fig_name = f'{env.__class__.__name__}-{target}-do({\",\".join(interventions.keys())})'\n",
    "\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "if save_plots:\n",
    "    plt.savefig(fig_dir + f'{figdate}-{fig_name}.{fig_format}', dpi=dpi, bbox_inches='tight')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
